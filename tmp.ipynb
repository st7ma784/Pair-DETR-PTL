{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "@torch.jit.script_if_tracing\n",
    "def MyLinearSumAssignment(TruthTensor, maximize=True,lookahead=2):\n",
    "    '''\n",
    "    If Maximize is False, I'm trying to minimize the costs. \n",
    "    This means that the mask must instead make all the weights far above all the others - 'inf' kind of thing. \n",
    "    '''\n",
    "    #assert truthtensor is 2d and nonzero\n",
    "    mask=torch.zeros_like(TruthTensor)\n",
    "    results=torch.zeros_like(TruthTensor,dtype=torch.bool)\n",
    "\n",
    "    finder=torch.argmax if maximize else torch.argmin\n",
    "    TruthTensor=TruthTensor-(torch.min(torch.min(TruthTensor)))\n",
    "    replaceval=-1 if maximize else (torch.max(torch.max(TruthTensor)))\n",
    "    #add a small amount of noise to the tensor to break ties\n",
    "    TruthTensor=TruthTensor+torch.randn_like(TruthTensor)*1e-6\n",
    "    dimsizes=torch.tensor(TruthTensor.shape)\n",
    "    #select index of the smallest value\n",
    "    bigdim=torch.argmax(dimsizes).item()   # 0 \n",
    "    small_dim=1-bigdim          # 1\n",
    "    \n",
    "    for i in range(TruthTensor.shape[small_dim]-1): # number of rows\n",
    "        \n",
    "        arr=torch.where(mask==1,replaceval,TruthTensor)\n",
    "        deltas=torch.diff(torch.topk(arr,lookahead,dim=bigdim,largest=maximize).values,n=lookahead-1,dim=bigdim)\n",
    "        col_index=torch.argmax(torch.abs(deltas),dim=small_dim) #this is the column to grab,  Note this measures step so its not important to do argmin...\n",
    "        row_index=finder(torch.select(arr,small_dim,col_index))\n",
    "        torch.select(mask,small_dim,col_index).fill_(1)\n",
    "        torch.select(mask,bigdim,row_index).fill_(1)\n",
    "\n",
    "        torch.select(torch.select(results,small_dim,col_index),0,row_index).fill_(True)\n",
    "        # plt.subplot(1,3,1)\n",
    "        # plt.imshow(arr.detach().cpu().numpy())\n",
    "        # plt.subplot(1,3,2)\n",
    "        # plt.imshow(mask.detach().cpu().numpy())\n",
    "        # plt.subplot(1,3,3)\n",
    "        # plt.imshow(results.detach().cpu().numpy())\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "    return torch.logical_or(results,torch.logical_not(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5572/10000 [00:15<00:12, 357.76it/s]"
     ]
    }
   ],
   "source": [
    "#do trials \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "for i in tqdm(range(10000)):\n",
    "\n",
    "    rand_input=torch.rand(10,10,requires_grad=True,device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mask=torch.rand(10,10,requires_grad=True,device='cuda' if torch.cuda.is_available() else 'cpu')>0.5\n",
    "\n",
    "    rand_input=torch.where(mask,rand_input,torch.zeros_like(rand_input))\n",
    "    torch.select(rand_input,0,0).fill_(0)\n",
    "    torch.select(rand_input,1,1).fill_(0)\n",
    "    results=MyLinearSumAssignment(rand_input, maximize=True,lookahead=2)\n",
    "    assert torch.all(torch.sum(results,dim=0)==1)\n",
    "    assert torch.all(torch.sum(results,dim=1)==1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
