diff --git a/data/VisGenomeDataModuleDETIC.py b/data/VisGenomeDataModuleDETIC.py
index 64e0b31..81548f7 100644
--- a/data/VisGenomeDataModuleDETIC.py
+++ b/data/VisGenomeDataModuleDETIC.py
@@ -7,6 +7,35 @@ import os
 import pytorch_lightning as pl
 from transformers import CLIPTokenizer
 import time
+import os
+import logging
+import json
+import numpy
+import joblib
+import sys
+import cv2
+import tempfile
+import cog
+import time
+from cog import Input,File,Path 
+from PIL import Image
+# import some common detectron2 utilities
+from detectron2.engine import DefaultPredictor
+from detectron2.config import get_cfg
+from detectron2.utils.visualizer import Visualizer
+from detectron2.data import MetadataCatalog
+import torch
+import os 
+# Detic libraries
+sys.path.insert(0, 'third_party/CenterNet2/')
+from centernet.config import add_centernet_config
+from detic.config import add_detic_config
+from detic.modeling.utils import reset_cls_test
+from detic.modeling.text.text_encoder import build_text_encoder
+import wget
+from typing import Optional,List
+# This file defines how the DETIC model is exposed as an AZUREML model endpoint. 
+# We have a couple of options here for this. 
 
 import base64,json,io
 from base64 import b64decode
@@ -79,19 +108,25 @@ class VisGenomeIterDataset(torch.utils.data.Dataset):
         return self.data.__iter__()
 
 class VisGenomeDataset(torch.utils.data.Dataset):
-    def __init__(self, tokenizer,split="train",dir="HF",T=prep):
+    def __init__(self, tokenizer,mask_predictor,split="train",dir="HF",T=prep):
         #print('Loading COCO dataset')
         self.data=load_dataset("visual_genome", "relationships_v1.2.0",streaming=False,cache_dir=dir)[split] #,download_mode="force_redownload"
         print("got datast")
+        self.mask_predictor=mask_predictor
         self.data = self.data.map(self.process)
         self.__getitem__ = self.data.__getitem__
         self.T=T
         self.tokenizer = tokenizer
         self.tokenize=lambda x:self.tokenizer(x,return_tensors="pt",padding="max_length", truncation=True,max_length=77)['input_ids']
     def process(self,item):
-        r=random.choice(item["relationships"])
+        #for all the relationships in the image
+        #  extract the classes of the subject, object 
+        #  predictions=self.predict(image,[subject,object])
+        #  check predictions for the masks within bboxes. 
+        #  if there is a mask, then add the relationship to the list of relationships
+        #  return the list of relationships tokenized and clip embedded. 
 
-        url     = 'http://localhost:5001/predictions'  #Point at the docker running on the same VM
+        #  return coco style annotations per ovject.
         Classes= " ,".join(list(set([*[r["subject"]["names"][0],r["predicate"],r["object"]["names"][0]]])))
         payload = json.dumps({ "input":{
             "image": item["url"],    #URL to image to search
@@ -126,6 +161,81 @@ class VisGenomeDataModule(pl.LightningDataModule):
         self.T=T
         self.tokenizer=CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32",cache_dir=Cache_dir)
 
+        self.cfg = get_cfg()
+        add_centernet_config(self.cfg)
+        add_detic_config(self.cfg)
+        self.cfg.merge_from_file("configs/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.yaml")
+        if not os.path.exists("./models/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth"):
+            url = 'https://dl.fbaipublicfiles.com/detic/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth'
+            filename = wget.download(url)
+            print("fetched to {}".format(filename))
+            self.cfg.MODEL.WEIGHTS = filename
+        else:
+            self.cfg.MODEL.WEIGHTS = './models/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth'
+        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
+        self.cfg.MODEL.ROI_BOX_HEAD.ZEROSHOT_WEIGHT_PATH = 'rand'
+        self.cfg.MODEL.ROI_HEADS.ONE_CLASS_PER_PROPOSAL = True
+        self.cfg.MODEL.DEVICE='cpu' 
+        self.text_encoder = build_text_encoder(pretrain=True)
+        self.text_encoder.eval()
+        self.predictor = DefaultPredictor(self.cfg)
+        self.model=self.predictor.model
+        BUILDIN_CLASSIFIER = {
+            'lvis': 'datasets/metadata/lvis_v1_clip_a+cname.npy',
+            'objects365': 'datasets/metadata/o365_clip_a+cnamefix.npy',
+            'openimages': 'datasets/metadata/oid_clip_a+cname.npy',
+            'coco': 'datasets/metadata/coco_clip_a+cname.npy',
+        }
+        BUILDIN_METADATA_PATH = {
+            'lvis': 'lvis_v1_val',
+            'objects365': 'objects365_v2_val',
+            'openimages': 'oid_val_expanded',
+            'coco': 'coco_2017_val',
+        }
+        
+        logging.info("Init complete")
+    
+
+    def predict(self,image,classes): 
+            
+            #assert custom_vocabulary is not None and len(custom_vocabulary.split(',')) > 0
+            metadata = MetadataCatalog.get(str(time.time()))
+            metadata.thing_classes = classes
+            classifier = self.get_clip_embeddings(metadata.thing_classes)
+            num_classes = len(classes)
+            self.model.roi_heads.num_classes = num_classes
+    
+            zs_weight = classifier
+            zs_weight = torch.cat(
+                [zs_weight, zs_weight.new_zeros((zs_weight.shape[0], 1))], 
+                dim=1) # D x (C + 1)
+            if self.model.roi_heads.box_predictor[0].cls_score.norm_weight:
+                zs_weight = torch.nn.functional.normalize(zs_weight, p=2, dim=0)
+            zs_weight = zs_weight.to(self.model.device)
+            for k in range(len(self.model.roi_heads.box_predictor)):
+                del self.model.roi_heads.box_predictor[k].cls_score.zs_weight
+                self.model.roi_heads.box_predictor[k].cls_score.zs_weight = zs_weight
+            # Reset visualization threshold
+            output_score_threshold = 0.3
+            for cascade_stages in range(len(self.predictor.model.roi_heads.box_predictor)):
+                self.predictor.model.roi_heads.box_predictor[cascade_stages].test_score_thresh = output_score_threshold
+
+            outputs = self.predictor(image)
+ 
+            #So - Idea - What if I could use the score to add noise to the output class. 
+            return dict(boxes=outputs['instances'].get_fields()["pred_boxes"].tensor,
+                        masks=outputs['instances'].get_fields()["pred_masks"],
+                        scores=outputs['instances'].get_fields()["scores"],
+                        pred_classes=outputs['instances'].get_fields()["pred_classes"])
+
+
+    def get_clip_embeddings(self,vocabulary, prompt='a '):
+        
+        texts = [prompt + x for x in vocabulary]
+        if "{}" in prompt:
+            texts=[prompt.format(x) for x in vocabulary]
+        return self.text_encoder(texts).detach().permute(1, 0).contiguous()
+    
     def train_dataloader(self, B=None):
         if B is None:
             B=self.batch_size 
@@ -147,9 +257,9 @@ class VisGenomeDataModule(pl.LightningDataModule):
         #print("Entered COCO datasetup")
         
         if stage == 'fit' or stage is None:
-            self.train=VisGenomeDataset(tokenizer=self.tokenizer,T=self.T,split="train")
-            self.val=VisGenomeDataset(tokenizer=self.tokenizer,T=self.T,split="dev")
-        self.test=VisGenomeDataset(tokenizer=self.tokenizer,T=self.T,split="test")
+            self.train=VisGenomeDataset(tokenizer=self.tokenizer,mask_predictor=self.predict, T=self.T,split="train")
+            self.val=VisGenomeDataset(tokenizer=self.tokenizer,mask_predictor=self.predict,T=self.T,split="dev")
+        self.test=VisGenomeDataset(tokenizer=self.tokenizer,mask_predictor=self.predict,T=self.T,split="test")
 
 
 
@@ -173,5 +283,9 @@ if __name__ == "__main__":
 
 
 
+'''
+
+
+
 
 
diff --git a/data/coco.py b/data/coco.py
index 2361ce4..57fd35f 100755
--- a/data/coco.py
+++ b/data/coco.py
@@ -50,7 +50,7 @@ class CocoDetection(torchvision.datasets.CocoDetection):
             target["masks"]=self.mask_transform(mask)
 
         summed_mask=torch.sum(target["masks"],dim=0).bool().int()
-        
+        #print("target",target)
         
         return img, target, self.tokenized_classnames,summed_mask
 
diff --git a/main.py b/main.py
index 9a0e3bd..c70defa 100755
--- a/main.py
+++ b/main.py
@@ -91,6 +91,7 @@ class PairDETR(pl.LightningModule):
 
         self.weight_dict = {'loss_out_iou':1,
                             'loss_gt_iou':1,
+                            'class_loss':1,
                             "class_mask_loss":2,
                        'loss_bbox_acc': 0.001*args['bbox_loss_coef'],
                        'loss_giou': 0.01*args['giou_loss_coef'],
@@ -301,11 +302,17 @@ class PairDETR(pl.LightningModule):
 if __name__ == '__main__':
     from argparser import get_args_parser
     import argparse
+    import wandb
     parser = argparse.ArgumentParser('Conditional DETR training and evaluation script', parents=[get_args_parser()])
     args = parser.parse_args()
     if args.output_dir:
         Path(args.output_dir).mkdir(parents=True, exist_ok=True)
-    
+    #make wandb logger
+    run=wandb.init(project="SPARC",entity="st7ma784",name="VRE",config=args)
+
+    logtool= pl.loggers.WandbLogger( project="SPARC",entity="st7ma784",experiment=run, save_dir=dir)
+
+    #wandb_logger = WandbLogger(project='pairdetr',entity="st7ma784",log_model=True)
     from data.coco import COCODataModule
     data=COCODataModule(Cache_dir=args.coco_path,batch_size=4)
     #convert to dict
@@ -316,7 +323,7 @@ if __name__ == '__main__':
                          max_epochs=20,#args['epochs'], 
                          num_sanity_val_steps=0,
                          gradient_clip_val=0.25,
-                         #accumulate_grad_batches=1,
+                         accumulate_grad_batches=1,
                          #callbacks=[ModelCheckpoint(dirpath=args['output_dir'],save_top_k=1,monitor='val_loss',mode='min')],
                          accelerator='auto',
                          fast_dev_run=False,  
diff --git a/model.py b/model.py
index f17660d..19c3bf6 100644
--- a/model.py
+++ b/model.py
@@ -675,8 +675,11 @@ class FastCriterion(nn.Module):
         # print(out_bbox_scores.shape) 
         # print(tgt_bbox.shape) # 211,4
         tgt_bbox=torch.flatten(tgt_bbox,0,1)
-        output_bbox=output_bbox[torchvision.ops.boxes.nms(output_bbox ,scores=torch.flatten(out_bbox_scores,0),iou_threshold=0.5)]
+        nms_indexes=torchvision.ops.boxes.nms(output_bbox ,scores=torch.flatten(out_bbox_scores,0),iou_threshold=0.5)
         
+
+        output_bbox=output_bbox[nms_indexes]
+
         ###########DO BOX Loss################
       
         #find best ious,
@@ -687,16 +690,16 @@ class FastCriterion(nn.Module):
 
         #raw sum - low is good
 
-        iou_total=torch.sum(-torch.sub(iou_scores,1)) #[]
+        # iou_total=torch.sum(-torch.sub(iou_scores,1)) #[]
 
         ''' Its super important do do bboxes of output onto truth and vice versa - otherwise we might just learn a single class
         Also worth highlighting that gumbel_softmax is a differentiable approximation of argmax,
           so we can backprop through it,but it's also therefore very noisy, so we need to be careful
         '''
-        # #softmax takes log probs, so we need to convert to log probs
+        #softmax takes log probs, so we need to convert to log probs
         out_log_iou_scores=torch.nn.functional.softmax(iou_scores,dim=1) #46,211
         out_one_hot=torch.nn.functional.gumbel_softmax(out_log_iou_scores,dim=1,hard=True).to(tgt_bbox) #46,211
-        #out_selected_boxes=torch.einsum("AB,NA->NB",tgt_bbox,out_one_hot).to(tgt_bbox) #211,4
+        out_selected_boxes=torch.einsum("AB,NA->NB",tgt_bbox,out_one_hot).to(tgt_bbox) #211,4
         out_selected_boxes=out_one_hot@tgt_bbox #46,4
         # #shapes are [46,4] and [211,4]
         # print(selected_boxes.shape,output_bbox.shape)
@@ -712,17 +715,15 @@ class FastCriterion(nn.Module):
         gt_ious=torch.diag(torchvision.ops.box_iou(
            gt_selected_boxes,tgt_bbox))
         gt_ious_total=-torch.sub(gt_ious,1)
-        # ##########################################################################
-        # #To Test: Use the similarity matric rather than the lookuptable??? 
-        # ##########################################################################
-        #I don't think I want to use NMS here, nor a lookuptable, because near-miss BBboxes will be very useful
-
-        # The Goal is a B,C,W,H mask.
-        # for Each box (B,Q) we want to find the best class (C) and the best output (W,H)
-        # we then want to use that boxes' logits to the class encoding to amount of mask to use
-        # class similarity is therefore shape B,Q,C  (B,Q,512)@(512,C) -> B,Q,C
-        # and we have BQWH masks, so we can use Q to get a resultant sum of B,C,W,H masks I.E a mask for each class in the batch
-        
+
+        ##### class loss
+        out_embs=gt_one_hot.T@torch.flatten(outputs["pred_logits"],0,1)[nms_indexes]
+        class_loss=tgt_embs@out_embs.T 
+        classlossT=out_embs@tgt_embs.T
+        class_loss=self.ce_loss(class_loss,torch.arange(tgt_embs.shape[0],device=out_embs.device))
+        classlossT=self.ce_loss(classlossT,torch.arange(tgt_embs.shape[0],device=out_embs.device))
+        class_loss=torch.mean(class_loss+classlossT)
+
         output_masks=outputs['pred_masks']#B,Q,W,H
         
         ###########DO MASK LOSS################
@@ -759,8 +760,7 @@ class FastCriterion(nn.Module):
         # ##############do dice loss################
 
         inputs = torch.flatten(output_class_masks,1)
-        inputs=self.relu(inputs)
-        targets = self.relu(torch.flatten(gt_class_masks,1))
+        targets = torch.flatten(gt_class_masks,1)
 
         dice_loss = torch.mean(torch.div(torch.sum(torch.mul(inputs,targets)), torch.add(torch.sum(inputs,-1),torch.sum(targets,-1)).add(1e-6)))
         # # # ##############do sigmoid focal loss################
@@ -771,13 +771,15 @@ class FastCriterion(nn.Module):
         # # sig_loss = torch.mul(ce_loss,torch.pow(1 - p_t,2))
         # #print("sig_loss",sig_loss)
         return {
+            #
+            "class_loss": class_loss,
             "class_mask_loss": dice_loss,
             "loss_mask": immask_loss/ output_bbox.shape[0], #(src_masks, masks),
              "loss_dice": overall_mask_loss/ output_bbox.shape[0], #(src_masks, masks, ),
             'loss_gt_iou': gt_ious_total.sum()/output_bbox.shape[0], # rename these later
-            #'loss_out_iou': out_iou_total.sum()/output_bbox.shape[0], # rename these later
+            'loss_out_iou': out_iou_total.sum()/output_bbox.shape[0], # rename these later
             'loss_bbox_acc': F.l1_loss(gt_selected_boxes, tgt_bbox, reduction='none').sum() / output_bbox.shape[0],
-            'loss_giou': iou_total / output_bbox.shape[0],
+            # 'loss_giou': iou_total / output_bbox.shape[0],
         },torch.flatten(predicted_classes,0,1)
     
 
